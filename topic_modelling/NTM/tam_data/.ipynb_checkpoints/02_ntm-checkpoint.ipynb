{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "\n",
    "Copy of 01_ntm due to inference using model.tar.gz was not working\n",
    "\n",
    "Source: \n",
    "1. https://github.com/aws/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb\n",
    "2. https://sagemaker-examples.readthedocs.io/en/latest/introduction_to_amazon_algorithms/ntm_synthetic/ntm_synthetic.html#Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import dask.dataframe as dd\n",
    "import tempfile\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.float_format', str)\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "working_dir = '/home/ec2-user/SageMaker'\n",
    "base_dir = '/home/ec2-user/SageMaker/topic_modelling/'\n",
    "# s3_data_path = 's3://bucket-sushant/bangla-character-recognition/'\n",
    "\n",
    "## For reproducible results\n",
    "seed_value = 18\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "# https://stackoverflow.com/questions/5836335/consistently-create-same-random-numpy-array/5837352#5837352\n",
    "random_state = np.random.RandomState(seed=seed_value)\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def read_from_s3(file_path):\n",
    "    bucket_name = file_path.split('/')[2]\n",
    "    key = '/'.join(file_path.split('/')[3:])\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    body = response['Body'].read()\n",
    "    return body\n",
    "\n",
    "def read_pickle_from_s3(file_path):\n",
    "    data = pickle.loads(read_from_s3(file_path))\n",
    "    return data\n",
    "\n",
    "# def read_csv_from_s3(file_path):\n",
    "#     data = pd.read_csv(file_path, low_memory=False)\n",
    "#     return data\n",
    "\n",
    "def store_object_to_s3_as_pickle(data, file_path):\n",
    "    bucket_name = file_path.split('/')[2]\n",
    "    key = '/'.join(file_path.split('/')[3:])\n",
    "#     # uses lot of memory\n",
    "#     pickle_obj = pickle.dumps(data)\n",
    "#     return s3.put_object(Key=key, Bucket=bucket_name, Body=pickle_obj)\n",
    "    # using tmp file\n",
    "    fd, path = tempfile.mkstemp()\n",
    "    try:\n",
    "        with open(path, 'wb') as pointer:\n",
    "            pickle.dump(data, pointer)\n",
    "        with open(path, \"rb\") as pointer:\n",
    "            s3.upload_fileobj(pointer, bucket_name, key)\n",
    "    finally:\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory:  /home/ec2-user/SageMaker/topic_modelling/NTM/tam_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def check_create_dir(dir):\n",
    "    if os.path.exists(dir):  # cleanup existing data folder\n",
    "        shutil.rmtree(dir)\n",
    "    os.mkdir(dir)\n",
    "    \n",
    "    \n",
    "data_dir = f'{base_dir}/Data'\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current directory: \", current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12099, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>service</th>\n",
       "      <th>case_billing_region</th>\n",
       "      <th>customer_billing_country_name</th>\n",
       "      <th>comm_owner_agent_login</th>\n",
       "      <th>comm_body</th>\n",
       "      <th>case_creation_cal_date</th>\n",
       "      <th>comm_date_utc</th>\n",
       "      <th>comm_subject</th>\n",
       "      <th>case_severity</th>\n",
       "      <th>urls</th>\n",
       "      <th>4_stop_words_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8415574821</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>gupmanav</td>\n",
       "      <td>Dear Team,  I am facing issue in one of the EC...</td>\n",
       "      <td>6/2/2021 0:00</td>\n",
       "      <td>6/2/2021 0:00</td>\n",
       "      <td>1/2 checks EC2</td>\n",
       "      <td>4</td>\n",
       "      <td>https://genpact.zoom.us/my/talati</td>\n",
       "      <td>dear team facing issue one ec2 coming kindly r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7938887601</td>\n",
       "      <td>Axiata Digital Services Sdn Bhd</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>arizona</td>\n",
       "      <td>Hello,  We haven't heard back from you regardi...</td>\n",
       "      <td>2/1/2021 0:00</td>\n",
       "      <td>2/9/2021 0:00</td>\n",
       "      <td>1/2 status check</td>\n",
       "      <td>4</td>\n",
       "      <td>https://console.aws.amazon.com/support/home?#/...</td>\n",
       "      <td>hello heard back regarding case continued supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7938887601</td>\n",
       "      <td>Axiata Digital Services Sdn Bhd</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>wadhwh</td>\n",
       "      <td>while checking the instance the above instance...</td>\n",
       "      <td>2/1/2021 0:00</td>\n",
       "      <td>2/1/2021 0:00</td>\n",
       "      <td>1/2 status check</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>checking instance instance status check would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7938887601</td>\n",
       "      <td>Axiata Digital Services Sdn Bhd</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>wadhwh</td>\n",
       "      <td>Hello,   We have looked into the issue and we ...</td>\n",
       "      <td>2/1/2021 0:00</td>\n",
       "      <td>2/9/2021 0:00</td>\n",
       "      <td>1/2 status check</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>hello looked issue taking action towards think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7745171561</td>\n",
       "      <td>Tata Communications Ltd.</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>arizona</td>\n",
       "      <td>Please let us know if we helped resolve your i...</td>\n",
       "      <td>12/16/2020 0:00</td>\n",
       "      <td>12/26/2020 0:00</td>\n",
       "      <td>2/2 check failed</td>\n",
       "      <td>3</td>\n",
       "      <td>https://console.aws.amazon.com/support/feedbac...</td>\n",
       "      <td>please let know helped resolve issue yes click...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      case_id                    customer_name  \\\n",
       "0  8415574821                          Genpact   \n",
       "1  7938887601  Axiata Digital Services Sdn Bhd   \n",
       "2  7938887601  Axiata Digital Services Sdn Bhd   \n",
       "3  7938887601  Axiata Digital Services Sdn Bhd   \n",
       "4  7745171561         Tata Communications Ltd.   \n",
       "\n",
       "                               service case_billing_region  \\\n",
       "0  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "1  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "2  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "3  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "4  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "\n",
       "  customer_billing_country_name comm_owner_agent_login  \\\n",
       "0                         INDIA               gupmanav   \n",
       "1                         INDIA                arizona   \n",
       "2                         INDIA                 wadhwh   \n",
       "3                         INDIA                 wadhwh   \n",
       "4                         INDIA                arizona   \n",
       "\n",
       "                                           comm_body case_creation_cal_date  \\\n",
       "0  Dear Team,  I am facing issue in one of the EC...          6/2/2021 0:00   \n",
       "1  Hello,  We haven't heard back from you regardi...          2/1/2021 0:00   \n",
       "2  while checking the instance the above instance...          2/1/2021 0:00   \n",
       "3  Hello,   We have looked into the issue and we ...          2/1/2021 0:00   \n",
       "4  Please let us know if we helped resolve your i...        12/16/2020 0:00   \n",
       "\n",
       "     comm_date_utc      comm_subject  case_severity  \\\n",
       "0    6/2/2021 0:00    1/2 checks EC2              4   \n",
       "1    2/9/2021 0:00  1/2 status check              4   \n",
       "2    2/1/2021 0:00  1/2 status check              4   \n",
       "3    2/9/2021 0:00  1/2 status check              4   \n",
       "4  12/26/2020 0:00  2/2 check failed              3   \n",
       "\n",
       "                                                urls  \\\n",
       "0                  https://genpact.zoom.us/my/talati   \n",
       "1  https://console.aws.amazon.com/support/home?#/...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4  https://console.aws.amazon.com/support/feedbac...   \n",
       "\n",
       "                                4_stop_words_removed  \n",
       "0  dear team facing issue one ec2 coming kindly r...  \n",
       "1  hello heard back regarding case continued supp...  \n",
       "2  checking instance instance status check would ...  \n",
       "3  hello looked issue taking action towards think...  \n",
       "4  please let know helped resolve issue yes click...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(f'{data_dir}/topic_modeling_data.pkl')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10889, 13), (1210, 13))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data.sample(frac=0.9, random_state=seed_value)\n",
    "test = data.loc[~data.index.isin(train.index)]\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_pickle(f'{data_dir}/topic_modeling_data_test.pkl')\n",
    "train.to_pickle(f'{data_dir}/topic_modeling_data_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(f'{data_dir}/topic_modeling_data_test.pkl')\n",
    "train = pd.read_pickle(f'{data_dir}/topic_modeling_data_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_list = list(train['4_stop_words_removed'].values)\n",
    "test_doc_list = list(test['4_stop_words_removed'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 10889, list, 1210)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_doc_list), len(train_doc_list), type(test_doc_list), len(test_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stack arn aws cloudformation region wa trying update launch configuration created stack use volume ami amazon linux stack update failed error invalid valid volume type standard try launch instance using ami via ec2 console able missing',\n",
       " 'hope great instance configured aws cli written script download file bucket run script manually terminal work working crontab')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_list[1], test_doc_list[1], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing and counting, this may take a few minutes...\n",
      "vocab size: 10355\n",
      "Done. Time elapsed: 0.52s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Lemmatizing and counting, this may take a few minutes...\")\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(\n",
    "    input=\"content\",\n",
    "    analyzer=\"word\",\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_doc_list)\n",
    "test_vectors = vectorizer.transform(test_doc_list)\n",
    "\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "vocab_size = len(vocab_list)\n",
    "print(\"vocab size:\", vocab_size)\n",
    "print(\"Done. Time elapsed: {:.2f}s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scipy.sparse.csr.csr_matrix,\n",
       " (10889, 10355),\n",
       " ['aadministrative',\n",
       "  'aakarsh',\n",
       "  'aamir',\n",
       "  'aashish',\n",
       "  'aathira',\n",
       "  'ababababbaab',\n",
       "  'abbrev',\n",
       "  'abbreviated',\n",
       "  'abdallah',\n",
       "  'abdul'],\n",
       " 10355)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_vectors), train_vectors.shape, vocab_list[:10], vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n",
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "\n",
    "def setting_sparse_dtypes(vectors):\n",
    "    vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "    print(type(vectors), vectors.dtype)\n",
    "    return vectors\n",
    "\n",
    "\n",
    "train_vectors_sparse = setting_sparse_dtypes(train_vectors)\n",
    "test_vectors_sparse = setting_sparse_dtypes(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to /home/ec2-user/SageMaker/topic_modelling//Data/train/train_part0.pbr\n",
      "Saved data to /home/ec2-user/SageMaker/topic_modelling//Data/train/train_part1.pbr\n",
      "Saved data to /home/ec2-user/SageMaker/topic_modelling//Data/train/train_part2.pbr\n",
      "Saved data to /home/ec2-user/SageMaker/topic_modelling//Data/train/train_part3.pbr\n",
      "Saved data to /home/ec2-user/SageMaker/topic_modelling//Data/test/test_part0.pbr\n"
     ]
    }
   ],
   "source": [
    "def split_convert(sparray, prefix, fname_template=\"data_part{}.pbr\", n_parts=2):\n",
    "    import io\n",
    "    import sagemaker.amazon.common as smac\n",
    "\n",
    "    chunk_size = sparray.shape[0] // n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size\n",
    "        if i + 1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "\n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        with open(fname, \"wb\") as f:\n",
    "            f.write(buf.getvalue())\n",
    "        print(\"Saved data to {}\".format(fname))\n",
    "\n",
    "\n",
    "train_data_dir = os.path.join(data_dir, \"train\")\n",
    "test_data_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "check_create_dir(train_data_dir)\n",
    "check_create_dir(test_data_dir)\n",
    "\n",
    "split_convert(train_vectors_sparse, prefix=train_data_dir, fname_template=\"train_part{}.pbr\", n_parts=4)\n",
    "split_convert(test_vectors_sparse, prefix=test_data_dir, fname_template=\"test_part{}.pbr\", n_parts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the vocabulary file\n",
    "To make use of the auxiliary channel for vocabulary file, we first save the text file with the name `vocab.txt` in the auxiliary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_data_dir = os.path.join(data_dir, \"auxiliary\")\n",
    "check_create_dir(aux_data_dir)\n",
    "with open(os.path.join(aux_data_dir, \"vocab.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in vocab_list:\n",
    "        f.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set location s3://bucket-sushant/topic_modelling/ntm_2/train\n",
      "Auxiliary data location s3://bucket-sushant/topic_modelling/ntm_2/auxiliary\n",
      "Test data location s3://bucket-sushant/topic_modelling/ntm_2/test\n",
      "Trained model will be saved at s3://bucket-sushant/topic_modelling/ntm_2/output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = 'bucket-sushant'  # <or insert your own bucket name>#\n",
    "prefix = \"topic_modelling/ntm_2/\"\n",
    "\n",
    "train_prefix = os.path.join(prefix, \"train\")\n",
    "aux_prefix = os.path.join(prefix, \"auxiliary\")\n",
    "test_prefix = os.path.join(prefix, \"test\")\n",
    "output_prefix = os.path.join(prefix, \"output\")\n",
    "\n",
    "s3_train_data = os.path.join(\"s3://\", bucket, train_prefix)\n",
    "s3_aux_data = os.path.join(\"s3://\", bucket, aux_prefix)\n",
    "s3_test_data = os.path.join(\"s3://\", bucket, test_prefix)\n",
    "output_path = os.path.join(\"s3://\", bucket, output_prefix)\n",
    "print(\"Training set location\", s3_train_data)\n",
    "print(\"Auxiliary data location\", s3_aux_data)\n",
    "print(\"Test data location\", s3_test_data)\n",
    "print(\"Trained model will be saved at\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the input directories to s3\n",
    "We use the `aws` command line interface (CLI) to upload the various input channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'Completed 256.0 KiB/2.7 MiB (4.4 MiB/s) with 4 file(s) remaining\\rCompleted 512.0 KiB/2.7 MiB (8.5 MiB/s) with 4 file(s) remaining\\rCompleted 768.0 KiB/2.7 MiB (12.4 MiB/s) with 4 file(s) remaining\\rCompleted 1.0 MiB/2.7 MiB (16.2 MiB/s) with 4 file(s) remaining  \\rCompleted 1.2 MiB/2.7 MiB (20.0 MiB/s) with 4 file(s) remaining  \\rCompleted 1.5 MiB/2.7 MiB (23.0 MiB/s) with 4 file(s) remaining  \\rCompleted 1.8 MiB/2.7 MiB (26.5 MiB/s) with 4 file(s) remaining  \\rCompleted 2.0 MiB/2.7 MiB (28.6 MiB/s) with 4 file(s) remaining  \\rCompleted 2.2 MiB/2.7 MiB (14.5 MiB/s) with 4 file(s) remaining  \\rupload: ../../Data/train/train_part3.pbr to s3://bucket-sushant/topic_modelling/ntm_2/train/train_part3.pbr\\nCompleted 2.2 MiB/2.7 MiB (14.5 MiB/s) with 3 file(s) remaining\\rCompleted 2.4 MiB/2.7 MiB (15.1 MiB/s) with 3 file(s) remaining\\rupload: ../../Data/train/train_part1.pbr to s3://bucket-sushant/topic_modelling/ntm_2/train/train_part1.pbr\\nCompleted 2.4 MiB/2.7 MiB (15.1 MiB/s) with 2 file(s) remaining\\rCompleted 2.5 MiB/2.7 MiB (15.6 MiB/s) with 2 file(s) remaining\\rupload: ../../Data/train/train_part0.pbr to s3://bucket-sushant/topic_modelling/ntm_2/train/train_part0.pbr\\nCompleted 2.5 MiB/2.7 MiB (15.6 MiB/s) with 1 file(s) remaining\\rCompleted 2.7 MiB/2.7 MiB (15.7 MiB/s) with 1 file(s) remaining\\rupload: ../../Data/train/train_part2.pbr to s3://bucket-sushant/topic_modelling/ntm_2/train/train_part2.pbr\\n',\n",
       " None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd_train = \"aws s3 cp \" + train_data_dir + \" \" + s3_train_data + \" --recursive\"\n",
    "p = subprocess.Popen(cmd_train, shell=True, stdout=subprocess.PIPE)\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'Completed 256.0 KiB/288.6 KiB (5.2 MiB/s) with 1 file(s) remaining\\rCompleted 288.6 KiB/288.6 KiB (2.5 MiB/s) with 1 file(s) remaining\\rupload: ../../Data/test/test_part0.pbr to s3://bucket-sushant/topic_modelling/ntm_2/test/test_part0.pbr\\n',\n",
       " None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_test = \"aws s3 cp \" + test_data_dir + \" \" + s3_test_data + \" --recursive\"\n",
    "p = subprocess.Popen(cmd_test, shell=True, stdout=subprocess.PIPE)\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'Completed 89.5 KiB/89.5 KiB (1.1 MiB/s) with 1 file(s) remaining\\rupload: ../../Data/auxiliary/vocab.txt to s3://bucket-sushant/topic_modelling/ntm_2/auxiliary/vocab.txt\\n',\n",
       " None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_aux = \"aws s3 cp \" + aux_data_dir + \" \" + s3_aux_data + \" --recursive\"\n",
    "p = subprocess.Popen(cmd_aux, shell=True, stdout=subprocess.PIPE)\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, \"ntm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ntm = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 15\n",
    "ntm.set_hyperparameters(\n",
    "    num_topics=num_topics, feature_dim=vocab_size, mini_batch_size=60, epochs=50, sub_sample=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "The class sagemaker.session.s3_input has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import s3_input\n",
    "\n",
    "s3_train = s3_input(\n",
    "    s3_train_data, distribution=\"ShardedByS3Key\", content_type=\"application/x-recordio-protobuf\"\n",
    ")\n",
    "s3_test = s3_input(\n",
    "    s3_test_data, distribution=\"FullyReplicated\", content_type=\"application/x-recordio-protobuf\"\n",
    ")\n",
    "\n",
    "s3_aux = s3_input(s3_aux_data, distribution=\"FullyReplicated\", content_type=\"text/plain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-02 07:34:18 Starting - Starting the training job...\n",
      "2021-09-02 07:34:41 Starting - Launching requested ML instancesProfilerReport-1630568058: InProgress\n",
      "......\n",
      "2021-09-02 07:35:42 Starting - Preparing the instances for training.........\n",
      "2021-09-02 07:37:03 Downloading - Downloading input data\n",
      "2021-09-02 07:37:03 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '10355', 'num_topics': '15', 'sub_sample': '0.8', 'epochs': '50', 'mini_batch_size': '60'}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '60', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '0.8', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '10355', 'num_topics': '15'}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] nvidia-smi: took 0.028 seconds to run.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] Using default worker.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:37:56.557] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] Initializing\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] vocab.txt\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:37:56 INFO 139965026494272] Loading pre-trained token embedding vectors from /opt/amazon/lib/python3.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\n",
      "2021-09-02 07:38:02 Training - Training image download completed. Training in progress.\u001b[34m[09/02/2021 07:38:09 WARNING 139965026494272] 3218 out of 10355 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:09 INFO 139965026494272] Vocab embedding shape: (10355, 50)\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:09 INFO 139965026494272] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:09 INFO 139965026494272] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568289.7402663, \"EndTime\": 1630568289.7402956, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:38:09.740] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 13190, \"num_examples\": 1, \"num_bytes\": 14872}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:09 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:09 INFO 139965026494272] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:38:15.824] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 6079, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] # Finished training epoch 1 on 8880 examples from 148 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] Subsampled 148 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] Loss (name: value) total: 6.659919773136173\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] Loss (name: value) kld: 0.20390366219856718\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] Loss (name: value) recons: 6.4560160971976615\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] Loss (name: value) logppx: 6.659919773136173\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=6.659919773136173\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] Timing: train: 6.09s, val: 0.00s, epoch: 6.09s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568289.7409818, \"EndTime\": 1630568295.8305233, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Total Batches Seen\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1788.101154903204 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:15 INFO 139965026494272] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:38:22.149] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 6318, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] # Finished training epoch 2 on 8249 examples from 138 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] Subsampled 138 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] Loss (name: value) total: 5.812012280818921\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] Loss (name: value) kld: 0.2025699982320629\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] Loss (name: value) recons: 5.609442287942637\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] Loss (name: value) logppx: 5.812012280818921\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=5.812012280818921\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] Timing: train: 6.32s, val: 0.01s, epoch: 6.33s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568295.830858, \"EndTime\": 1630568302.1570003, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 21778.0, \"count\": 1, \"min\": 21778, \"max\": 21778}, \"Total Batches Seen\": {\"sum\": 364.0, \"count\": 1, \"min\": 364, \"max\": 364}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1721.2260507889293 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:22 INFO 139965026494272] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:38:28.146] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 5988, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] # Finished training epoch 3 on 8669 examples from 145 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] Subsampled 145 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] Loss (name: value) total: 5.597849394699623\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] Loss (name: value) kld: 0.19982030358807795\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] Loss (name: value) recons: 5.3980291028954515\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] Loss (name: value) logppx: 5.597849394699623\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=5.597849394699623\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] Timing: train: 5.99s, val: 0.00s, epoch: 6.00s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568302.1573288, \"EndTime\": 1630568308.1527212, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 32667.0, \"count\": 1, \"min\": 32667, \"max\": 32667}, \"Total Batches Seen\": {\"sum\": 546.0, \"count\": 1, \"min\": 546, \"max\": 546}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1816.1822365992973 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:28 INFO 139965026494272] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:38:34.189] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 6035, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] # Finished training epoch 4 on 8669 examples from 145 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] Subsampled 145 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] Loss (name: value) total: 5.542954739976203\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] Loss (name: value) kld: 0.19750054869158515\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] Loss (name: value) recons: 5.345454196272225\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] Loss (name: value) logppx: 5.542954739976203\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=5.542954739976203\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] patience losses:[6.659919773136173, 5.812012280818921, 5.597849394699623] min patience loss:5.597849394699623 current loss:5.542954739976203 absolute loss difference:0.054894654723420366\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] Timing: train: 6.04s, val: 0.00s, epoch: 6.04s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568308.1530175, \"EndTime\": 1630568314.195953, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 43556.0, \"count\": 1, \"min\": 43556, \"max\": 43556}, \"Total Batches Seen\": {\"sum\": 728.0, \"count\": 1, \"min\": 728, \"max\": 728}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1801.892211346674 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:34 INFO 139965026494272] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:38:40.098] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 5901, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] # Finished training epoch 5 on 8460 examples from 141 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] Subsampled 141 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] Loss (name: value) total: 5.47650712466409\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] Loss (name: value) kld: 0.20184276357610176\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] Loss (name: value) recons: 5.27466432106974\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] Loss (name: value) logppx: 5.47650712466409\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=5.47650712466409\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] patience losses:[5.812012280818921, 5.597849394699623, 5.542954739976203] min patience loss:5.542954739976203 current loss:5.47650712466409 absolute loss difference:0.06644761531211252\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] Timing: train: 5.90s, val: 0.01s, epoch: 5.91s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568314.1962888, \"EndTime\": 1630568320.1056545, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 54445.0, \"count\": 1, \"min\": 54445, \"max\": 54445}, \"Total Batches Seen\": {\"sum\": 910.0, \"count\": 1, \"min\": 910, \"max\": 910}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1842.612117033393 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:40 INFO 139965026494272] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:38:46.367] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 6261, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] # Finished training epoch 6 on 8969 examples from 150 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] Subsampled 150 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] Loss (name: value) total: 5.438219000922309\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] Loss (name: value) kld: 0.20108183092541165\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] Loss (name: value) recons: 5.2371371782090925\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] Loss (name: value) logppx: 5.438219000922309\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=5.438219000922309\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] patience losses:[5.597849394699623, 5.542954739976203, 5.47650712466409] min patience loss:5.47650712466409 current loss:5.438219000922309 absolute loss difference:0.03828812374178181\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] Timing: train: 6.26s, val: 0.00s, epoch: 6.27s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568320.1059914, \"EndTime\": 1630568326.3734634, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 65334.0, \"count\": 1, \"min\": 65334, \"max\": 65334}, \"Total Batches Seen\": {\"sum\": 1092.0, \"count\": 1, \"min\": 1092, \"max\": 1092}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1737.3427881376704 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:46 INFO 139965026494272] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:38:52.767] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 6393, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] # Finished training epoch 7 on 8969 examples from 150 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] Subsampled 150 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] Loss (name: value) total: 5.414559529622395\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] Loss (name: value) kld: 0.20587478235032824\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] Loss (name: value) recons: 5.208684753417969\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] Loss (name: value) logppx: 5.414559529622395\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=5.414559529622395\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] patience losses:[5.542954739976203, 5.47650712466409, 5.438219000922309] min patience loss:5.438219000922309 current loss:5.414559529622395 absolute loss difference:0.023659471299913193\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] Timing: train: 6.40s, val: 0.00s, epoch: 6.40s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568326.3737228, \"EndTime\": 1630568332.7735872, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 76223.0, \"count\": 1, \"min\": 76223, \"max\": 76223}, \"Total Batches Seen\": {\"sum\": 1274.0, \"count\": 1, \"min\": 1274, \"max\": 1274}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1701.4038161223182 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:52 INFO 139965026494272] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:38:58.947] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 6173, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] # Finished training epoch 8 on 8849 examples from 148 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] Subsampled 148 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] Loss (name: value) total: 5.41959285736084\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] Loss (name: value) kld: 0.20381686778755875\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] Loss (name: value) recons: 5.2157760053067594\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] Loss (name: value) logppx: 5.41959285736084\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=5.41959285736084\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] patience losses:[5.47650712466409, 5.438219000922309, 5.414559529622395] min patience loss:5.414559529622395 current loss:5.41959285736084 absolute loss difference:0.005033327738444449\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] Timing: train: 6.18s, val: 0.00s, epoch: 6.18s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568332.7738605, \"EndTime\": 1630568338.9502532, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 87112.0, \"count\": 1, \"min\": 87112, \"max\": 87112}, \"Total Batches Seen\": {\"sum\": 1456.0, \"count\": 1, \"min\": 1456, \"max\": 1456}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1762.9579707788052 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:38:58 INFO 139965026494272] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:39:05.279] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 6328, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] # Finished training epoch 9 on 8700 examples from 145 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] Subsampled 145 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] Loss (name: value) total: 5.417415978442664\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] Loss (name: value) kld: 0.2090108804593141\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] Loss (name: value) recons: 5.2084050917351385\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] Loss (name: value) logppx: 5.417415978442664\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=5.417415978442664\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] patience losses:[5.438219000922309, 5.414559529622395, 5.41959285736084] min patience loss:5.414559529622395 current loss:5.417415978442664 absolute loss difference:0.0028564488202684757\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] Timing: train: 6.33s, val: 0.00s, epoch: 6.33s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568338.9505768, \"EndTime\": 1630568345.2812319, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 98001.0, \"count\": 1, \"min\": 98001, \"max\": 98001}, \"Total Batches Seen\": {\"sum\": 1638.0, \"count\": 1, \"min\": 1638, \"max\": 1638}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1720.002933569909 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:05 INFO 139965026494272] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:39:11.983] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 6701, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] # Finished training epoch 10 on 8849 examples from 148 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] Subsampled 148 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] Loss (name: value) total: 5.392329148988466\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] Loss (name: value) kld: 0.21347021406835265\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] Loss (name: value) recons: 5.178858904795604\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] Loss (name: value) logppx: 5.392329148988466\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=5.392329148988466\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] patience losses:[5.414559529622395, 5.41959285736084, 5.417415978442664] min patience loss:5.414559529622395 current loss:5.392329148988466 absolute loss difference:0.022230380633929236\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] Timing: train: 6.70s, val: 0.00s, epoch: 6.71s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568345.2815466, \"EndTime\": 1630568351.989546, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 108890.0, \"count\": 1, \"min\": 108890, \"max\": 108890}, \"Total Batches Seen\": {\"sum\": 1820.0, \"count\": 1, \"min\": 1820, \"max\": 1820}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1623.2421412386095 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:11 INFO 139965026494272] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:39:18.154] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 6164, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] # Finished training epoch 11 on 8969 examples from 150 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] Subsampled 150 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] Loss (name: value) total: 5.377408513387044\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] Loss (name: value) kld: 0.21591251521640353\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] Loss (name: value) recons: 5.161496003892687\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] Loss (name: value) logppx: 5.377408513387044\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=5.377408513387044\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] patience losses:[5.41959285736084, 5.417415978442664, 5.392329148988466] min patience loss:5.392329148988466 current loss:5.377408513387044 absolute loss difference:0.014920635601422205\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] Timing: train: 6.17s, val: 0.01s, epoch: 6.17s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568351.9899669, \"EndTime\": 1630568358.1618242, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 119779.0, \"count\": 1, \"min\": 119779, \"max\": 119779}, \"Total Batches Seen\": {\"sum\": 2002.0, \"count\": 1, \"min\": 2002, \"max\": 2002}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1764.2473952674814 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:18 INFO 139965026494272] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:39:24.367] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 6205, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] # Finished training epoch 12 on 8609 examples from 144 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] Subsampled 144 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] Loss (name: value) total: 5.389773451840436\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] Loss (name: value) kld: 0.22030234298220389\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] Loss (name: value) recons: 5.169471104939778\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] Loss (name: value) logppx: 5.389773451840436\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=5.389773451840436\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] patience losses:[5.417415978442664, 5.392329148988466, 5.377408513387044] min patience loss:5.377408513387044 current loss:5.389773451840436 absolute loss difference:0.01236493845339215\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] Timing: train: 6.21s, val: 0.00s, epoch: 6.21s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568358.162158, \"EndTime\": 1630568364.3703938, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 130668.0, \"count\": 1, \"min\": 130668, \"max\": 130668}, \"Total Batches Seen\": {\"sum\": 2184.0, \"count\": 1, \"min\": 2184, \"max\": 2184}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1753.9157326766178 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:24 INFO 139965026494272] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:39:30.790] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 6419, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] # Finished training epoch 13 on 9029 examples from 151 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] Subsampled 151 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] Loss (name: value) total: 5.368180545697412\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] Loss (name: value) kld: 0.22657332999290483\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] Loss (name: value) recons: 5.141607228125431\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] Loss (name: value) logppx: 5.368180545697412\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=5.368180545697412\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] patience losses:[5.392329148988466, 5.377408513387044, 5.389773451840436] min patience loss:5.377408513387044 current loss:5.368180545697412 absolute loss difference:0.009227967689631633\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] Timing: train: 6.42s, val: 0.01s, epoch: 6.43s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568364.370723, \"EndTime\": 1630568370.7981808, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 141557.0, \"count\": 1, \"min\": 141557, \"max\": 141557}, \"Total Batches Seen\": {\"sum\": 2366.0, \"count\": 1, \"min\": 2366, \"max\": 2366}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1694.0934912581017 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:30 INFO 139965026494272] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:39:37.011] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 6212, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] # Finished training epoch 14 on 8909 examples from 149 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] Subsampled 149 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] Loss (name: value) total: 5.344288683044297\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] Loss (name: value) kld: 0.22683111058518923\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] Loss (name: value) recons: 5.117457575446007\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] Loss (name: value) logppx: 5.344288683044297\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=5.344288683044297\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] patience losses:[5.377408513387044, 5.389773451840436, 5.368180545697412] min patience loss:5.368180545697412 current loss:5.344288683044297 absolute loss difference:0.023891862653115048\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] Timing: train: 6.21s, val: 0.00s, epoch: 6.22s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568370.7985337, \"EndTime\": 1630568377.016718, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 152446.0, \"count\": 1, \"min\": 152446, \"max\": 152446}, \"Total Batches Seen\": {\"sum\": 2548.0, \"count\": 1, \"min\": 2548, \"max\": 2548}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1751.1141401740915 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:37 INFO 139965026494272] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:39:43.092] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 6074, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] # Finished training epoch 15 on 8820 examples from 147 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] Subsampled 147 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] Loss (name: value) total: 5.354803397596018\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] Loss (name: value) kld: 0.2298507156285569\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] Loss (name: value) recons: 5.124952697753907\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] Loss (name: value) logppx: 5.354803397596018\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=5.354803397596018\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] patience losses:[5.389773451840436, 5.368180545697412, 5.344288683044297] min patience loss:5.344288683044297 current loss:5.354803397596018 absolute loss difference:0.010514714551720594\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] Timing: train: 6.08s, val: 0.00s, epoch: 6.08s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568377.0170095, \"EndTime\": 1630568383.093968, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 163335.0, \"count\": 1, \"min\": 163335, \"max\": 163335}, \"Total Batches Seen\": {\"sum\": 2730.0, \"count\": 1, \"min\": 2730, \"max\": 2730}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1791.803871683742 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:43 INFO 139965026494272] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:39:49.298] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 6203, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] # Finished training epoch 16 on 8940 examples from 149 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] Subsampled 149 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] Loss (name: value) total: 5.35961317023975\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] Loss (name: value) kld: 0.23655202255419702\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] Loss (name: value) recons: 5.12306116091325\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] Loss (name: value) logppx: 5.35961317023975\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=5.35961317023975\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] patience losses:[5.368180545697412, 5.344288683044297, 5.354803397596018] min patience loss:5.344288683044297 current loss:5.35961317023975 absolute loss difference:0.01532448719545254\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] Timing: train: 6.21s, val: 0.00s, epoch: 6.21s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568383.094285, \"EndTime\": 1630568389.3002956, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 174224.0, \"count\": 1, \"min\": 174224, \"max\": 174224}, \"Total Batches Seen\": {\"sum\": 2912.0, \"count\": 1, \"min\": 2912, \"max\": 2912}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1754.546064723905 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:49 INFO 139965026494272] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:39:55.735] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 6434, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] # Finished training epoch 17 on 8969 examples from 150 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] Subsampled 150 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] Loss (name: value) total: 5.333971694946289\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] Loss (name: value) kld: 0.2380511278046502\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] Loss (name: value) recons: 5.095920581393772\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] Loss (name: value) logppx: 5.333971694946289\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=5.333971694946289\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] patience losses:[5.344288683044297, 5.354803397596018, 5.35961317023975] min patience loss:5.344288683044297 current loss:5.333971694946289 absolute loss difference:0.010316988098008295\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] Timing: train: 6.44s, val: 0.01s, epoch: 6.44s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568389.300597, \"EndTime\": 1630568395.7431445, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 185113.0, \"count\": 1, \"min\": 185113, \"max\": 185113}, \"Total Batches Seen\": {\"sum\": 3094.0, \"count\": 1, \"min\": 3094, \"max\": 3094}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1690.1240590886134 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:39:55 INFO 139965026494272] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:40:02.707] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 6963, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] # Finished training epoch 18 on 9389 examples from 157 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] Subsampled 157 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] Loss (name: value) total: 5.3403991132278605\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] Loss (name: value) kld: 0.24421528327237269\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] Loss (name: value) recons: 5.096183830309825\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] Loss (name: value) logppx: 5.3403991132278605\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=5.3403991132278605\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] patience losses:[5.354803397596018, 5.35961317023975, 5.333971694946289] min patience loss:5.333971694946289 current loss:5.3403991132278605 absolute loss difference:0.006427418281571562\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] Timing: train: 6.97s, val: 0.00s, epoch: 6.97s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568395.7434776, \"EndTime\": 1630568402.7093778, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 196002.0, \"count\": 1, \"min\": 196002, \"max\": 196002}, \"Total Batches Seen\": {\"sum\": 3276.0, \"count\": 1, \"min\": 3276, \"max\": 3276}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1563.1535955498232 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:02 INFO 139965026494272] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:40:09.722] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 7010, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] # Finished training epoch 19 on 9029 examples from 151 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] Subsampled 151 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] Loss (name: value) total: 5.314045880382951\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] Loss (name: value) kld: 0.24827317923110054\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] Loss (name: value) recons: 5.065772735730438\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] Loss (name: value) logppx: 5.314045880382951\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=5.314045880382951\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] patience losses:[5.35961317023975, 5.333971694946289, 5.3403991132278605] min patience loss:5.333971694946289 current loss:5.314045880382951 absolute loss difference:0.019925814563338307\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] Timing: train: 7.01s, val: 0.01s, epoch: 7.02s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568402.7096322, \"EndTime\": 1630568409.7302272, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 206891.0, \"count\": 1, \"min\": 206891, \"max\": 206891}, \"Total Batches Seen\": {\"sum\": 3458.0, \"count\": 1, \"min\": 3458, \"max\": 3458}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1550.977628247057 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:09 INFO 139965026494272] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:40:15.900] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 6169, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] # Finished training epoch 20 on 8729 examples from 146 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] Subsampled 146 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] Loss (name: value) total: 5.300325355355598\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] Loss (name: value) kld: 0.2511401484001717\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] Loss (name: value) recons: 5.049185229436447\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] Loss (name: value) logppx: 5.300325355355598\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=5.300325355355598\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] patience losses:[5.333971694946289, 5.3403991132278605, 5.314045880382951] min patience loss:5.314045880382951 current loss:5.300325355355598 absolute loss difference:0.013720525027352437\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] Timing: train: 6.17s, val: 0.01s, epoch: 6.18s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568409.7305396, \"EndTime\": 1630568415.9079056, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 217780.0, \"count\": 1, \"min\": 217780, \"max\": 217780}, \"Total Batches Seen\": {\"sum\": 3640.0, \"count\": 1, \"min\": 3640, \"max\": 3640}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1762.6786647141973 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:15 INFO 139965026494272] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:40:21.983] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 6074, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] # Finished training epoch 21 on 8549 examples from 143 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] Subsampled 143 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] Loss (name: value) total: 5.333051805118303\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] Loss (name: value) kld: 0.2529261197799291\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] Loss (name: value) recons: 5.080125729576413\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] Loss (name: value) logppx: 5.333051805118303\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=5.333051805118303\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] patience losses:[5.3403991132278605, 5.314045880382951, 5.300325355355598] min patience loss:5.300325355355598 current loss:5.333051805118303 absolute loss difference:0.03272644976270467\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] Timing: train: 6.08s, val: 0.00s, epoch: 6.08s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568415.908306, \"EndTime\": 1630568421.9852154, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 228669.0, \"count\": 1, \"min\": 228669, \"max\": 228669}, \"Total Batches Seen\": {\"sum\": 3822.0, \"count\": 1, \"min\": 3822, \"max\": 3822}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1791.821024167732 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:21 INFO 139965026494272] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:40:27.905] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 5919, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] # Finished training epoch 22 on 8160 examples from 136 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] Subsampled 136 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] Loss (name: value) total: 5.349938613293218\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] Loss (name: value) kld: 0.25721983570678564\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] Loss (name: value) recons: 5.092718745212929\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] Loss (name: value) logppx: 5.349938613293218\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=5.349938613293218\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] patience losses:[5.314045880382951, 5.300325355355598, 5.333051805118303] min patience loss:5.300325355355598 current loss:5.349938613293218 absolute loss difference:0.049613257937619615\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] Timing: train: 5.92s, val: 0.00s, epoch: 5.92s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568421.9854825, \"EndTime\": 1630568427.9078615, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 239558.0, \"count\": 1, \"min\": 239558, \"max\": 239558}, \"Total Batches Seen\": {\"sum\": 4004.0, \"count\": 1, \"min\": 4004, \"max\": 4004}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1838.5704624350221 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:27 INFO 139965026494272] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:40:34.192] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 6284, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] # Finished training epoch 23 on 8849 examples from 148 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] Subsampled 148 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] Loss (name: value) total: 5.323601212372651\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] Loss (name: value) kld: 0.2623576511670877\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] Loss (name: value) recons: 5.061243574468939\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] Loss (name: value) logppx: 5.323601212372651\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=5.323601212372651\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] patience losses:[5.300325355355598, 5.333051805118303, 5.349938613293218] min patience loss:5.300325355355598 current loss:5.323601212372651 absolute loss difference:0.023275857017052815\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] Timing: train: 6.29s, val: 0.00s, epoch: 6.29s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568427.9081874, \"EndTime\": 1630568434.1948314, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 250447.0, \"count\": 1, \"min\": 250447, \"max\": 250447}, \"Total Batches Seen\": {\"sum\": 4186.0, \"count\": 1, \"min\": 4186, \"max\": 4186}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1732.0324364027126 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] \u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:34 INFO 139965026494272] # Starting training for epoch 24\u001b[0m\n",
      "\n",
      "2021-09-02 07:40:50 Uploading - Uploading generated training model\n",
      "2021-09-02 07:40:50 Completed - Training job completed\n",
      "\u001b[34m[2021-09-02 07:40:40.221] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 6026, \"num_examples\": 182, \"num_bytes\": 2817548}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] # Finished training epoch 24 on 8700 examples from 145 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Subsampled 145 batches out of 182 total batches.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Loss (name: value) total: 5.354549679810973\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Loss (name: value) kld: 0.2657469315364443\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Loss (name: value) recons: 5.088802753316945\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Loss (name: value) logppx: 5.354549679810973\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=5.354549679810973\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] patience losses:[5.333051805118303, 5.349938613293218, 5.323601212372651] min patience loss:5.323601212372651 current loss:5.354549679810973 absolute loss difference:0.030948467438322247\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Timing: train: 6.03s, val: 0.00s, epoch: 6.03s\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568434.195141, \"EndTime\": 1630568440.2235222, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 261336.0, \"count\": 1, \"min\": 261336, \"max\": 261336}, \"Total Batches Seen\": {\"sum\": 4368.0, \"count\": 1, \"min\": 4368, \"max\": 4368}, \"Max Records Seen Between Resets\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Max Batches Seen Between Resets\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 10889.0, \"count\": 1, \"min\": 10889, \"max\": 10889}, \"Number of Batches Since Last Reset\": {\"sum\": 182.0, \"count\": 1, \"min\": 182, \"max\": 182}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] #throughput_metric: host=algo-1, train throughput=1806.2450324656654 records/second\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 WARNING 139965026494272] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Best model based on early stopping at epoch 20. Best loss: 5.300325355355598\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Topics from epoch:final (num_topics:15) [wetc 0.25, tu 0.53]:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.39, 0.88] training incoming respond accept management sent corner cost star toll international return rated documentation mail experience managed enquiry center follow\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.33, 0.76] failed timestamp status type check showing getting failing reachability health launch login stopped unable tried server instance window volume password\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.22, 0.38] assistance ha contact need resolved phone amazon case reconnecting chat initiating join disconnected reconnect connect update parekodi discus anybody sanjib\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.14, 0.38] phone amazon reconnecting initiating lost chat connect alisha disconnected pfa mangaonkar update help asap urgently nimisha check ranjan looping pingable\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.41, 0.77] helped click yes resolve let know issue underlying cause check status hardware failure health caused wa observed facing healthy network\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.15, 0.39] phone amazon initiating chat connect pfa lost reinitiating disconnected update join kindly able team fortigate asap shardul session ciming initiate\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.17, 0.41] phone reconnecting amazon disconnected chat initiating connect join got mbaosxy help feroz team reopening pandey subham banner kindly helped anybody\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.24, 0.31] initiating phone reconnecting chat amazon reinitiating chime assistance disconnected team connect kindly update initiate asap need reconnect thanks help unable\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.14, 0.47] phone amazon disconnected chat himanshu reinitiating joshi connect got join ranpise initiating update kishore lost sumeet pinging istance assist shankar\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.30, 0.85] pleasure premium inconvenience underlying feel contacting hope happy conversation chatting corner incoming reviewed star rated experience ahead rating assisting talking\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.13, 0.36] phone amazon initiating chat pfa reconnecting connect reinitiating lost connecting mangaonkar update alisha team nilay parekodi joshi rohini pandey looping\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.22, 0.45] phone amazon reconnecting initiating status disconnected chat check connect incompatible exhausted failing failed reinitiating reachability involvement evident needful startup unable\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.19, 0.32] phone disconnected connect amazon got reachability reconnecting update reinitiating pfa chat initiating geeting login lost unable dear rdp check parekodi\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.22, 0.25] phone amazon disconnected chat reconnecting joshi help asap initiating himanshu connect update assistance helped click team resolve yes join anybody\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] [0.48, 0.93] heard continued wish mark close regarding resolved hour action open time case web want url required note best contact following\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Saved checkpoint to \"/tmp/tmpz35t8vw5/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:40:40.356] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 163798, \"num_examples\": 1, \"num_bytes\": 8700}\u001b[0m\n",
      "\u001b[34m[2021-09-02 07:40:40.692] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 335, \"num_examples\": 21, \"num_bytes\": 295560}\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Finished scoring on 1200 examples from 20 batches, each of size 60.\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Loss (name: value) total: 5.171790262858073\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Loss (name: value) kld: 0.2799549166361491\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Loss (name: value) recons: 4.89183536529541\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] Loss (name: value) logppx: 5.171790262858073\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568440.3559034, \"EndTime\": 1630568440.6927202, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1210.0, \"count\": 1, \"min\": 1210, \"max\": 1210}, \"Total Batches Seen\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Max Records Seen Between Resets\": {\"sum\": 1210.0, \"count\": 1, \"min\": 1210, \"max\": 1210}, \"Max Batches Seen Between Resets\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 1210.0, \"count\": 1, \"min\": 1210, \"max\": 1210}, \"Number of Batches Since Last Reset\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[09/02/2021 07:40:40 INFO 139965026494272] #test_score (algo-1) : ('log_perplexity', 5.171790262858073)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1630568276.5503216, \"EndTime\": 1630568440.6940074, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 13181.90050125122, \"count\": 1, \"min\": 13181.90050125122, \"max\": 13181.90050125122}, \"epochs\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"early_stop.time\": {\"sum\": 66.94221496582031, \"count\": 24, \"min\": 0.18906593322753906, \"max\": 6.005764007568359}, \"update.time\": {\"sum\": 150470.49045562744, \"count\": 24, \"min\": 5909.196615219116, \"max\": 7020.414590835571}, \"finalize.time\": {\"sum\": 112.78700828552246, \"count\": 1, \"min\": 112.78700828552246, \"max\": 112.78700828552246}, \"model.serialize.time\": {\"sum\": 18.69821548461914, \"count\": 1, \"min\": 18.69821548461914, \"max\": 18.69821548461914}, \"model.score.time\": {\"sum\": 336.75122261047363, \"count\": 1, \"min\": 336.75122261047363, \"max\": 336.75122261047363}, \"setuptime\": {\"sum\": 38.25855255126953, \"count\": 1, \"min\": 38.25855255126953, \"max\": 38.25855255126953}, \"totaltime\": {\"sum\": 164215.07358551025, \"count\": 1, \"min\": 164215.07358551025, \"max\": 164215.07358551025}}}\n",
      "\u001b[0m\n",
      "Training seconds: 239\n",
      "Billable seconds: 239\n"
     ]
    }
   ],
   "source": [
    "ntm.fit({\"train\": s3_train, \"auxiliary\": s3_aux, \"test\": s3_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: ntm-2021-09-02-07-34-18-125\n"
     ]
    }
   ],
   "source": [
    "print(\"Training job name: {}\".format(ntm.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "\n",
    "ntm_predictor.serializer = CSVSerializer()\n",
    "ntm_predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (1210, 10355))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_vectors = vectorizer.transform(test_doc_list).toarray()\n",
    "\n",
    "\n",
    "type(inference_vectors), inference_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'topic_weights': [0.0544995815, 0.0428887047, 0.048204124, 0.0501664542, 0.3033443093, 0.051579278, 0.049753841, 0.0511398762, 0.0517315976, 0.0460794829, 0.0473534539, 0.0509621464, 0.0495990925, 0.0483409949, 0.0543571934]}, {'topic_weights': [0.0598470457, 0.1547537148, 0.0339846462, 0.036045894, 0.0358615518, 0.0397222713, 0.0381161645, 0.0414611511, 0.0371201485, 0.3429684639, 0.035939537, 0.0365365706, 0.034672372, 0.0396570824, 0.0333133414]}, {'topic_weights': [0.1491554677, 0.0224514008, 0.0175683424, 0.0312191695, 0.018333748, 0.0324145332, 0.028275989, 0.0316344164, 0.0322962105, 0.0331618525, 0.0271863304, 0.0304296315, 0.032882452, 0.0256469455, 0.4873435199]}, {'topic_weights': [0.2285828292, 0.0632833615, 0.021175256, 0.0261117797, 0.0195404049, 0.0268449169, 0.0277151577, 0.0294546653, 0.0257418025, 0.4104583263, 0.0247723889, 0.0243954062, 0.0248954762, 0.0270667858, 0.0199614149]}, {'topic_weights': [0.1491554677, 0.0224514008, 0.0175683424, 0.0312191695, 0.018333748, 0.0324145332, 0.028275989, 0.0316344164, 0.0322962105, 0.0331618525, 0.0271863304, 0.0304296315, 0.032882452, 0.0256469455, 0.4873435199]}, {'topic_weights': [0.1991084516, 0.1573955268, 0.0293999817, 0.0352734663, 0.025395805, 0.0361252278, 0.0371456258, 0.0405624919, 0.0354244895, 0.2034345418, 0.0326736718, 0.0330086462, 0.0318556577, 0.0419775769, 0.0612189211]}, {'topic_weights': [0.1491554677, 0.0224514008, 0.0175683424, 0.0312191695, 0.018333748, 0.0324145332, 0.028275989, 0.0316344164, 0.0322962105, 0.0331618525, 0.0271863304, 0.0304296315, 0.032882452, 0.0256469455, 0.4873435199]}, {'topic_weights': [0.0374769606, 0.1292423457, 0.0330158584, 0.0358860604, 0.0442999005, 0.0392618403, 0.0382415019, 0.0398953184, 0.0378945805, 0.386439085, 0.0369499661, 0.037099123, 0.0340683386, 0.0414744541, 0.0287546422]}, {'topic_weights': [0.0544995815, 0.0428887047, 0.048204124, 0.0501664542, 0.3033443093, 0.051579278, 0.049753841, 0.0511398762, 0.0517315976, 0.0460794829, 0.0473534539, 0.0509621464, 0.0495990925, 0.0483409949, 0.0543571934]}, {'topic_weights': [0.0544995815, 0.0428887047, 0.048204124, 0.0501664542, 0.3033443093, 0.051579278, 0.049753841, 0.0511398762, 0.0517315976, 0.0460794829, 0.0473534539, 0.0509621464, 0.0495990925, 0.0483409949, 0.0543571934]}]}\n"
     ]
    }
   ],
   "source": [
    "results = ntm_predictor.predict(inference_vectors[:10], initial_args={\"ContentType\": \"text/csv\"})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batches(data, rows=10):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = []\n",
    "    for array in split_array:\n",
    "        results = ntm_predictor.predict(array, initial_args={\"ContentType\": \"text/csv\"})\n",
    "        predictions += [r[\"topic_weights\"] for r in results[\"predictions\"]]\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_batches(inference_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1210, 15) (1210,)\n"
     ]
    }
   ],
   "source": [
    "arg_max_results = np.argmax(results, axis=1)\n",
    "print(results.shape, arg_max_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "    0: \"training incoming respond accept management sent corner cost star toll international return rated documentation mail experience managed enquiry center follow\",\n",
    "    1: \"failed timestamp status type check showing getting failing reachability health launch login stopped unable tried server instance window volume password\",\n",
    "    2: \"assistance ha contact need resolved phone amazon case reconnecting chat initiating join disconnected reconnect connect update parekodi discus anybody sanjib\",\n",
    "    3: \"phone amazon reconnecting initiating lost chat connect alisha disconnected pfa mangaonkar update help asap urgently nimisha check ranjan looping pingable\",\n",
    "    4: \"helped click yes resolve let know issue underlying cause check status hardware failure health caused wa observed facing healthy network\",\n",
    "    5: \"phone amazon initiating chat connect pfa lost reinitiating disconnected update join kindly able team fortigate asap shardul session ciming initiate\",\n",
    "    6: \"phone reconnecting amazon disconnected chat initiating connect join got mbaosxy help feroz team reopening pandey subham banner kindly helped anybody\",\n",
    "    7: \"initiating phone reconnecting chat amazon reinitiating chime assistance disconnected team connect kindly update initiate asap need reconnect thanks help unable\",\n",
    "    8: \"phone amazon disconnected chat himanshu reinitiating joshi connect got join ranpise initiating update kishore lost sumeet pinging istance assist shankar\",\n",
    "    9: \"pleasure premium inconvenience underlying feel contacting hope happy conversation chatting corner incoming reviewed star rated experience ahead rating assisting talking\",\n",
    "    10: \"phone amazon initiating chat pfa reconnecting connect reinitiating lost connecting mangaonkar update alisha team nilay parekodi joshi rohini pandey looping\",\n",
    "    11: \"phone amazon reconnecting initiating status disconnected chat check connect incompatible exhausted failing failed reinitiating reachability involvement evident needful startup unable\",\n",
    "    12: \"phone disconnected connect amazon got reachability reconnecting update reinitiating pfa chat initiating geeting login lost unable dear rdp check parekodi\",\n",
    "    13: \"phone amazon disconnected chat reconnecting joshi help asap initiating himanshu connect update assistance helped click team resolve yes join anybody\",\n",
    "    14: \"heard continued wish mark close regarding resolved hour action open time case web want url required note best contact following\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predicted_topic_pos'] = arg_max_results\n",
    "test['predicted_topics'] = test.predicted_topic_pos.apply(lambda x: topics[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(f'{data_dir}/test_predictions_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>service</th>\n",
       "      <th>case_billing_region</th>\n",
       "      <th>customer_billing_country_name</th>\n",
       "      <th>comm_owner_agent_login</th>\n",
       "      <th>comm_body</th>\n",
       "      <th>case_creation_cal_date</th>\n",
       "      <th>comm_date_utc</th>\n",
       "      <th>comm_subject</th>\n",
       "      <th>case_severity</th>\n",
       "      <th>urls</th>\n",
       "      <th>4_stop_words_removed</th>\n",
       "      <th>predicted_topic_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12063</th>\n",
       "      <td>7577744251</td>\n",
       "      <td>Mylan</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>arizona</td>\n",
       "      <td>Please let us know if we helped resolve your i...</td>\n",
       "      <td>11/5/2020 0:00</td>\n",
       "      <td>11/15/2020 0:00</td>\n",
       "      <td>ua1cmtprd001 (i-88540e18) Server Down</td>\n",
       "      <td>1</td>\n",
       "      <td>https://console.aws.amazon.com/support/feedbac...</td>\n",
       "      <td>please let know helped resolve issue yes click...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12071</th>\n",
       "      <td>8588787621</td>\n",
       "      <td>Mylan</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>noljami</td>\n",
       "      <td>Case 8584651511 has been raised.Kindly help us...</td>\n",
       "      <td>7/15/2021 0:00</td>\n",
       "      <td>7/15/2021 0:00</td>\n",
       "      <td>ua5nbp002(i-01936a8b5870b7752) server got rebo...</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>case ha raised kindly help get</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12079</th>\n",
       "      <td>8122517891</td>\n",
       "      <td>Solutions Infini Technologies (India) Private ...</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>apeksh</td>\n",
       "      <td>Hi Apeksha,  Thank you so much for providing t...</td>\n",
       "      <td>3/18/2021 0:00</td>\n",
       "      <td>3/19/2021 0:00</td>\n",
       "      <td>unable to find the backup of the instance</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>apeksha thank much providing information need ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12080</th>\n",
       "      <td>8122517891</td>\n",
       "      <td>Solutions Infini Technologies (India) Private ...</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>apeksh</td>\n",
       "      <td>Hi,  Please provide the RCA for the health che...</td>\n",
       "      <td>3/18/2021 0:00</td>\n",
       "      <td>3/18/2021 0:00</td>\n",
       "      <td>unable to find the backup of the instance</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>please provide health check failure please fin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12100</th>\n",
       "      <td>7331463811</td>\n",
       "      <td>ALL_DEPRECATED</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>pauyen</td>\n",
       "      <td>VPC???IGW</td>\n",
       "      <td>8/31/2020 0:00</td>\n",
       "      <td>9/1/2020 0:00</td>\n",
       "      <td>????ip</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>vpc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          case_id                                      customer_name  \\\n",
       "12063  7577744251                                              Mylan   \n",
       "12071  8588787621                                              Mylan   \n",
       "12079  8122517891  Solutions Infini Technologies (India) Private ...   \n",
       "12080  8122517891  Solutions Infini Technologies (India) Private ...   \n",
       "12100  7331463811                                     ALL_DEPRECATED   \n",
       "\n",
       "                                   service case_billing_region  \\\n",
       "12063  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "12071  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "12079  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "12080  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "12100  Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "\n",
       "      customer_billing_country_name comm_owner_agent_login  \\\n",
       "12063                         INDIA                arizona   \n",
       "12071                         INDIA                noljami   \n",
       "12079                         INDIA                 apeksh   \n",
       "12080                         INDIA                 apeksh   \n",
       "12100                         INDIA                 pauyen   \n",
       "\n",
       "                                               comm_body  \\\n",
       "12063  Please let us know if we helped resolve your i...   \n",
       "12071  Case 8584651511 has been raised.Kindly help us...   \n",
       "12079  Hi Apeksha,  Thank you so much for providing t...   \n",
       "12080  Hi,  Please provide the RCA for the health che...   \n",
       "12100                                          VPC???IGW   \n",
       "\n",
       "      case_creation_cal_date    comm_date_utc  \\\n",
       "12063         11/5/2020 0:00  11/15/2020 0:00   \n",
       "12071         7/15/2021 0:00   7/15/2021 0:00   \n",
       "12079         3/18/2021 0:00   3/19/2021 0:00   \n",
       "12080         3/18/2021 0:00   3/18/2021 0:00   \n",
       "12100         8/31/2020 0:00    9/1/2020 0:00   \n",
       "\n",
       "                                            comm_subject  case_severity  \\\n",
       "12063              ua1cmtprd001 (i-88540e18) Server Down              1   \n",
       "12071  ua5nbp002(i-01936a8b5870b7752) server got rebo...              1   \n",
       "12079          unable to find the backup of the instance              5   \n",
       "12080          unable to find the backup of the instance              5   \n",
       "12100                                             ????ip              3   \n",
       "\n",
       "                                                    urls  \\\n",
       "12063  https://console.aws.amazon.com/support/feedbac...   \n",
       "12071                                               None   \n",
       "12079                                               None   \n",
       "12080                                               None   \n",
       "12100                                               None   \n",
       "\n",
       "                                    4_stop_words_removed  predicted_topic_pos  \n",
       "12063  please let know helped resolve issue yes click...                    4  \n",
       "12071                     case ha raised kindly help get                    1  \n",
       "12079  apeksha thank much providing information need ...                    1  \n",
       "12080  please provide health check failure please fin...                    1  \n",
       "12100                                                vpc                    1  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>service</th>\n",
       "      <th>case_billing_region</th>\n",
       "      <th>customer_billing_country_name</th>\n",
       "      <th>comm_owner_agent_login</th>\n",
       "      <th>comm_body</th>\n",
       "      <th>case_creation_cal_date</th>\n",
       "      <th>comm_date_utc</th>\n",
       "      <th>comm_subject</th>\n",
       "      <th>case_severity</th>\n",
       "      <th>urls</th>\n",
       "      <th>4_stop_words_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7621010241</td>\n",
       "      <td>Mylan</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Windows)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>arizona</td>\n",
       "      <td>Please let us know if we helped resolve your i...</td>\n",
       "      <td>11/16/2020 0:00</td>\n",
       "      <td>11/17/2020 0:00</td>\n",
       "      <td>ASG Failed - IP limit</td>\n",
       "      <td>4</td>\n",
       "      <td>https://console.aws.amazon.com/support/feedbac...</td>\n",
       "      <td>please let know helped resolve issue yes click...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7414621131</td>\n",
       "      <td>Tata Communications Ltd.</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>tchemvur</td>\n",
       "      <td>Hi,  Hope you are doing great. Instance ID: i-...</td>\n",
       "      <td>9/23/2020 0:00</td>\n",
       "      <td>9/23/2020 0:00</td>\n",
       "      <td>AWS CLI not working from Crontab</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>hope great instance configured aws cli written...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7682250211</td>\n",
       "      <td>Hotstar (Star TV India)</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>arizona</td>\n",
       "      <td>Hello,  We haven't heard back from you regardi...</td>\n",
       "      <td>12/2/2020 0:00</td>\n",
       "      <td>12/9/2020 0:00</td>\n",
       "      <td>About Amazon Corretto a OpenJDK distribution.</td>\n",
       "      <td>4</td>\n",
       "      <td>https://console.aws.amazon.com/support/home?#/...</td>\n",
       "      <td>hello heard back regarding case continued supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8302246421</td>\n",
       "      <td>ALL_DEPRECATED</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>anearin</td>\n",
       "      <td>Luke -  Thank you for taking a few minutes to ...</td>\n",
       "      <td>5/5/2021 0:00</td>\n",
       "      <td>5/18/2021 0:00</td>\n",
       "      <td>Additional Elastic IP Blocks for us-east-1, us...</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>luke thank taking minute chat today discussed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>8302246421</td>\n",
       "      <td>ALL_DEPRECATED</td>\n",
       "      <td>Elastic Compute Cloud (EC2 - Linux)</td>\n",
       "      <td>APAC</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>arizona</td>\n",
       "      <td>Hello,  We haven't heard back from you regardi...</td>\n",
       "      <td>5/5/2021 0:00</td>\n",
       "      <td>5/13/2021 0:00</td>\n",
       "      <td>Additional Elastic IP Blocks for us-east-1, us...</td>\n",
       "      <td>4</td>\n",
       "      <td>https://console.aws.amazon.com/support/home?#/...</td>\n",
       "      <td>hello heard back regarding case continued supp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       case_id             customer_name  \\\n",
       "29  7621010241                     Mylan   \n",
       "35  7414621131  Tata Communications Ltd.   \n",
       "41  7682250211   Hotstar (Star TV India)   \n",
       "44  8302246421            ALL_DEPRECATED   \n",
       "46  8302246421            ALL_DEPRECATED   \n",
       "\n",
       "                                  service case_billing_region  \\\n",
       "29  Elastic Compute Cloud (EC2 - Windows)                APAC   \n",
       "35    Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "41    Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "44    Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "46    Elastic Compute Cloud (EC2 - Linux)                APAC   \n",
       "\n",
       "   customer_billing_country_name comm_owner_agent_login  \\\n",
       "29                         INDIA                arizona   \n",
       "35                         INDIA               tchemvur   \n",
       "41                         INDIA                arizona   \n",
       "44                         INDIA                anearin   \n",
       "46                         INDIA                arizona   \n",
       "\n",
       "                                            comm_body case_creation_cal_date  \\\n",
       "29  Please let us know if we helped resolve your i...        11/16/2020 0:00   \n",
       "35  Hi,  Hope you are doing great. Instance ID: i-...         9/23/2020 0:00   \n",
       "41  Hello,  We haven't heard back from you regardi...         12/2/2020 0:00   \n",
       "44  Luke -  Thank you for taking a few minutes to ...          5/5/2021 0:00   \n",
       "46  Hello,  We haven't heard back from you regardi...          5/5/2021 0:00   \n",
       "\n",
       "      comm_date_utc                                       comm_subject  \\\n",
       "29  11/17/2020 0:00                              ASG Failed - IP limit   \n",
       "35   9/23/2020 0:00                   AWS CLI not working from Crontab   \n",
       "41   12/9/2020 0:00      About Amazon Corretto a OpenJDK distribution.   \n",
       "44   5/18/2021 0:00  Additional Elastic IP Blocks for us-east-1, us...   \n",
       "46   5/13/2021 0:00  Additional Elastic IP Blocks for us-east-1, us...   \n",
       "\n",
       "    case_severity                                               urls  \\\n",
       "29              4  https://console.aws.amazon.com/support/feedbac...   \n",
       "35              2                                               None   \n",
       "41              4  https://console.aws.amazon.com/support/home?#/...   \n",
       "44              4                                               None   \n",
       "46              4  https://console.aws.amazon.com/support/home?#/...   \n",
       "\n",
       "                                 4_stop_words_removed  \n",
       "29  please let know helped resolve issue yes click...  \n",
       "35  hope great instance configured aws cli written...  \n",
       "41  hello heard back regarding case continued supp...  \n",
       "44  luke thank taking minute chat today discussed ...  \n",
       "46  hello heard back regarding case continued supp...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.delete_model()\n",
    "ntm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
