{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Neural Topic Model now supports auxiliary vocabulary channel, new topic evaluation metrics, and training subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Neural Topic Model(NTM) is an unsupervised learning algorithm that learns the topic distributions of large collections of document corpus. With SageMaker NTM, you can build machine learning solutions for use cases such as document classification, information retrieval, and content recommendation. SageMaker provides a rich set of model training configuration options such as network architecture, automatic early stopping, as well as hyperparameters to fine-tune between a magnitude of metrics such as document modeling accuracy, human interpretability, and granularity of the learned topics.  see Introduction to the Amazon SageMaker Neural Topic Model (https://aws.amazon.com/blogs/machine-learning/introduction-to-the-amazon-sagemaker-neural-topic-model/) if you are not already familiar with SageMaker and SageMaker NTM.\n",
    "\n",
    "If you are new to machine learning, or want to free up time to focus on other tasks, then the fully automated Amazon Comprehend topic modeling API is the best option. If you are a data science specialist looking for finer control over the various layers of building and tuning your own topic modeling model, then the Amazon SageMaker NTM might work better for you. For example, let’s say you are building a document topic tagging application that needs a customized vocabulary, and you need the ability to adjust the algorithm hyperparameters, such as the number of layers of the neural network, so you can train a topic model that meets the target accuracy in terms of coherence and uniqueness scores. In this case, the Amazon SageMaker NTM would be the appropriate tool to use.\n",
    "\n",
    "In this blog, we want to introduce 3 new features of the SageMaker NTM that would help improve productivity, enhance topic coherence evaluation capability, and speed up model training. \n",
    "\n",
    "- Auxiliary vocabulary channel\n",
    "- Word Embedding Topic Coherence (WETC) and Topic Uniqueness (TU) metrics\n",
    "- Subsampling data during training\n",
    "\n",
    "In addition to these new features, by optimizing sparse operations and the parameter server, we have improved the speed of the algorithm by 2x for training and 4x for evaluation on a single GPU. The speedup is even more significant for multi-gpu training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Auxiliary vocabulary channel\n",
    "When an NTM training job runs, it outputs the training status and evaluation metrics to the CloudWatch logs. Among the outputs are lists of top words detected for each of the learned topics. Prior to the availability of auxiliary vocabulary channel support, the top words were represented as integers, and customers needed to map the integers to an external custom vocabulary lookup table in order to know what the actual words were. With the support of auxiliary vocabulary channel, users can now add a vocabulary file as an additional data input channel, and SageMaker NTM will output the actual words in a topic instead of integers. This feature eliminates the manual effort needed to map integers to the actual vocabulary. Below is a sample of what a custom vocabulary text file look like. The text file will simply contain a list of words, one word per row, in the order corresponding to the integer IDs provided in the data.\n",
    "```\n",
    "absent\n",
    "absentee\n",
    "absolute\n",
    "absolutely\n",
    "```\n",
    "\n",
    "To include an auxiliary vocabulary for a training job, you should name the vocabulary file `vocab.txt` and place it in the auxiliary channel. See the code example below for how to add the auxiliary vocabulary file. In this release we only support the UTF-8 encoding for the vocabulary file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Topic Coherence metrics\n",
    "To evaluate the performance of an trained SageMaker NTM model, customers can examine the perplexity metric emitted by the training job.  Sometimes, however customers also want to evaluate the topic coherence of a model that measures the closeness of the words in a topic. A good topic should have semantically similar words in it. Traditional methods like the Normalized Point-wise Mutual Information(NPMI), while widely accepted, require a large external corpus.  The new WETC metric measures the similarity of words in a topic by using a pre-trained word embedding, [Glove-6B-400K-50d](https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "Intuitively, each word in the vocabulary is given a vector representation (embedding). We compute the WETC of a topic by averaging the pair-wise cosine similarities between the vectors corresponding to the top words of the topic. Finally, we average the WETC for all the topics to obtain a single score for the model. \n",
    "\n",
    "Our tests have shown that WETC correlates very well with NPMI as an effective surrogate. For details about the pair-wise WETC computation and its correlation to NPMI, please refer to our paper [Coherence-Aware Neural Topic Modeling, Ding et. al. 2018 (Accepted for EMNLP 2018)](https://arxiv.org/pdf/1809.02687.pdf)\n",
    "\n",
    "WETC ranges between 0 and 1, higher is better. Typical value would be in the range of 0.2 to 0.8. The WETC metric is evaluated whenever the vocabulary file is provided. The average WETC score over the topics is displayed in the log above the top words of all topics. The WETC metric for each topic is also displayed along with the top words of each topic. Please refer to the screenshot below for an example.\n",
    "\n",
    "> Note in case many of the words in the supplied vocabulary cannot be found in the pre-trained word embedding, the WETC score can be misleading. Therefore we provide a warning message to alert the user exactly how many words in the vocabulary do not have an embedding:\n",
    "\n",
    "```\n",
    "[09/07/2018 14:18:57 WARNING 140296605947712] 69 out of 16648 in vocabulary do not have embeddings! Default vector used for unknown embedding!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Log with WETC metrics](WETC_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Uniqueness metric\n",
    "\n",
    "A good topic modeling algorithm should generate topics that are unique to avoid topic duplication. Customers who want to understand the topic uniqueness of a trained Amazon SageMaker NTM model to evaluate its quality can now use the new TU metric. \n",
    "To understand how TU works, suppose there are K topics and we extract the top n words for each topic, the TU for topic k is defined as\n",
    "![TU definition](TU_definition.png)\n",
    "\n",
    "The range of the TU value is between 1/K and 1, where K is the number of topics. A higher TU value represents higher topic uniqueness for the topics detected.\n",
    "\n",
    "The TU score is displayed regardless of the existence of a vocabulary file. Similar to the WETC, the average TU score over the topics is displayed in the log above the top words of all topics; the TU score for each topic is also displayed along with the top words of each topic. Please refer to the screenshot below for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Log with TU metrics](TU_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we introduce a new hyperparameter for subsampling the data during training\n",
    "\n",
    "## Subsampling data during training\n",
    "\n",
    "In typical online training, the entire training dataset is fed into the training algorithm for each epoch.  When the corpus is large, this leads to long training time.  With effective subsampling of the training dataset, we can achieve faster model convergence while maintaining the model performance.  The new subsampling feature of the SageMaker NMT allows customers to specify a percentage of training data used for training using a new hyperparameter, `sub_sample`.  For example, specifying 0.8 for `sub_sample` would direct SageMaker NTM to use 80% of training data randomly for each epoch. As a result, the algorithm will stochastically cover different subsets of data during different epochs. You can configure this value in both the SageMaker console or directly training code. See sample code below on how to set this value for training.\n",
    "\n",
    "```\n",
    "ntm.set_hyperparameters(num_topics=num_topics, feature_dim=vocab_size, mini_batch_size=128, \n",
    "                        epochs=100, sub_sample=0.7)\n",
    "```\n",
    "At the end of this notebook we will demonstrate that using subsampling can reduce the overall training time for large dataset and potentially achieve higher topic uniqueness and coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Finally, to illustrate the new features, we will go through an example with the public Wikitext launguage modeling dataset.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. The dataset can be downloaded from [here](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset). We will first use the wikitext-2 dataset. \n",
    "\n",
    "> **Acknowledgements:** Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Data Set\n",
    "\n",
    "First let's define the folder to hold the data and clean the content in it which might be from previous experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory:  /home/ec2-user/SageMaker/topic_modelling/NTM/ntm_topic_modeling_2021-09-01/wikitext-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def check_create_dir(dir):\n",
    "    if os.path.exists(dir):  # cleanup existing data folder\n",
    "        shutil.rmtree(dir)\n",
    "    os.mkdir(dir)\n",
    "\n",
    "\n",
    "dataset = \"wikitext-2\"\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, dataset)\n",
    "check_create_dir(data_dir)\n",
    "os.chdir(data_dir)\n",
    "print(\"Current directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can download and unzip the data. *Please review the following Acknowledgements, Copyright Information, and Availability notice before downloading the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4370k  100 4370k    0     0  34.1M      0 --:--:-- --:--:-- --:--:-- 34.1M\n",
      "Archive:  wikitext-2-v1.zip\n",
      "   creating: wikitext-2/\n",
      "  inflating: wikitext-2/wiki.test.tokens  \n",
      "  inflating: wikitext-2/wiki.valid.tokens  \n",
      "  inflating: wikitext-2/wiki.train.tokens  \n"
     ]
    }
   ],
   "source": [
    "# **Acknowledgements, Copyright Information, and Availability**\n",
    "# This dataset is available under the Creative Commons Attribution-ShareAlike License\n",
    "# Source: https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset\n",
    "\n",
    "!curl -O https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "!unzip wikitext-2-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of the `wiki.valid.tokens` is shown below. The datasets contains markdown text with all documents (articles) concatenated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    " = Homarus gammarus =\n",
    "\n",
    " Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into <unk> larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles .\n",
    "\n",
    " = = Description = =\n",
    "\n",
    " Homarus gammarus is a large <unk> , with a body length up to 60 centimetres ( 24 in ) and weighing up to 5 – 6 kilograms ( 11 – 13 lb ) , although the lobsters caught in lobster pots are usually 23 – 38 cm ( 9 – 15 in ) long and weigh 0 @.@ 7 – 2 @.@ 2 kg ( 1 @.@ 5 – 4 @.@ 9 lb ) . Like other crustaceans , lobsters have a hard <unk> which they must shed in order to grow , in a process called <unk> ( <unk> ) . This may occur several times a year for young lobsters , but decreases to once every 1 – 2 years for larger animals .\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We need to first parse the input files into separate documents. We can identify each document by its title in level-1 heading. Additional care is taken to check that the line containing the title should be sandwiched by blank lines to avoid false detection of document titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 documents parsed!\n",
      "60 documents parsed!\n",
      "60 documents parsed!\n"
     ]
    }
   ],
   "source": [
    "def is_document_start(line):\n",
    "    if len(line) < 4:\n",
    "        return False\n",
    "    if line[0] is \"=\" and line[-1] is \"=\":\n",
    "        if line[2] is not \"=\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def token_list_per_doc(input_dir, token_file):\n",
    "    lines_list = []\n",
    "    line_prev = \"\"\n",
    "    prev_line_start_doc = False\n",
    "    with open(os.path.join(input_dir, token_file), \"r\", encoding=\"utf-8\") as f:\n",
    "        for l in f:\n",
    "            line = l.strip()\n",
    "            if prev_line_start_doc and line:\n",
    "                # the previous line should not have been start of a document!\n",
    "                lines_list.pop()\n",
    "                lines_list[-1] = lines_list[-1] + \" \" + line_prev\n",
    "\n",
    "            if line:\n",
    "                if is_document_start(line) and not line_prev:\n",
    "                    lines_list.append(line)\n",
    "                    prev_line_start_doc = True\n",
    "                else:\n",
    "                    lines_list[-1] = lines_list[-1] + \" \" + line\n",
    "                    prev_line_start_doc = False\n",
    "            else:\n",
    "                prev_line_start_doc = False\n",
    "            line_prev = line\n",
    "\n",
    "    print(\"{} documents parsed!\".format(len(lines_list)))\n",
    "    return lines_list\n",
    "\n",
    "\n",
    "input_dir = os.path.join(data_dir, dataset)\n",
    "train_file = \"wiki.train.tokens\"\n",
    "val_file = \"wiki.valid.tokens\"\n",
    "test_file = \"wiki.test.tokens\"\n",
    "train_doc_list = token_list_per_doc(input_dir, train_file)\n",
    "val_doc_list = token_list_per_doc(input_dir, val_file)\n",
    "test_doc_list = token_list_per_doc(input_dir, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 600)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_doc_list), len(train_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'= Valkyria Chronicles III = Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game \\'s opening theme was sung by May \\'n . It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game \\'s expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . = = Gameplay = = As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through <unk> text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player \\'s approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game \\'s completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game \\'s two main <unk> , although they take a very minor role . The game \\'s battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action <unk> . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant <unk> to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become <unk> , while Imca can target multiple enemy units with her heavy weapon . Troops are divided into five classes : Scouts , <unk> , Engineers , <unk> and Armored Soldier . <unk> can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games \\' method of distributing to different unit types . = = Plot = = The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign <unk> , and military offenders whose real names are erased from the records and <unk> officially referred to by numbers . <unk> by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , <unk> <unk> , meaning \" Always Ready . \" The three main characters are <unk> Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace <unk> Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and <unk> Riela <unk> , a seemingly <unk> young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers . As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible <unk> in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , <unk> , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . <unk> by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless \\'s commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of <unk> in order to present evidence <unk> the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . <unk> due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian \\'s defeat , Dahau and Calamity Raven move to activate an ancient <unk> super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau \\'s last <unk> card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations \\' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the <unk> weapon . Each member then goes their separate ways in order to begin their lives <unk> . = = Development = = Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development beginning shortly after this . The director of Valkyria Chronicles II , Takeshi Ozawa , returned to that role for Valkyria Chronicles III . Development work took approximately one year . After the release of Valkyria Chronicles II , the staff took a look at both the popular response for the game and what they wanted to do next for the series . Like its predecessor , Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II , and they had not come up with the \" revolutionary \" idea that would warrant a new entry for the PlayStation 3 . Speaking in an interview , it was stated that the development team considered Valkyria Chronicles III to be the series \\' first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform . In addition to Sega staff from the previous games , development work was also handled by <unk> The original scenario was written <unk> <unk> , while the script was written by Hiroyuki <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> and <unk> <unk> . Its story was darker and more somber than that of its predecessor . The majority of material created for previous games , such as the <unk> system and the design of maps , was carried over . Alongside this , improvements were made to the game \\'s graphics and some elements were expanded , such as map layouts , mission structure , and the number of playable units per mission . A part of this upgrade involved creating unique <unk> models for each character \\'s body . In order to achieve this , the cooperative elements incorporated into the second game were removed , as they took up a large portion of memory space needed for the improvements . They also adjusted the difficulty settings and ease of play so they could appeal to new players while retaining the essential components of the series \\' gameplay . The newer systems were decided upon early in development . The character designs were done by <unk> Honjou , who had worked on the previous Valkyria Chronicles games . When creating the Nameless Squad , Honjou was faced with the same problem he had had during the first game : the military uniforms essentially destroyed character individuality , despite him needing to create unique characters the player could identify while maintaining a sense of reality within the Valkyria Chronicles world . The main color of the Nameless was black . As with the previous Valkyria games , Valkyria Chronicles III used the <unk> graphics engine . The anime opening was produced by Production I.G. = = = Music = = = The music was composed by Hitoshi Sakimoto , who had also worked on the previous Valkyria Chronicles games . When he originally heard about the project , he thought it would be a light tone similar to other Valkyria Chronicles games , but found the themes much darker than expected . An early theme he designed around his original vision of the project was rejected . He <unk> the main theme about seven times through the music production due to this need to <unk> the game . The main theme was initially recorded using orchestra , then Sakimoto removed elements such as the guitar and bass , then adjusted the theme using a synthesizer before <unk> segments such as the guitar piece on their own before incorporating them into the theme . The rejected main theme was used as a hopeful tune that played during the game \\'s ending . The battle themes were designed around the concept of a \" modern battle \" divorced from a fantasy scenario by using modern musical instruments , constructed to create a sense of <unk> . While Sakimoto was most used to working with synthesized music , he felt that he needed to incorporate live instruments such as orchestra and guitar . The guitar was played by <unk> <unk> , who also arranged several of the later tracks . The game \\'s opening theme song , \" If You Wish for ... \" ( <unk> , <unk> Kimi <unk> <unk> <unk> ) , was sung by Japanese singer May \\'n . Its theme was the reason soldiers fought , in particular their wish to protect what was precious to them rather than a sense of responsibility or duty . Its lyrics were written by <unk> <unk> , who had worked on May \\'n on previous singles . = = = Release = = = In September 2010 , a teaser website was revealed by Sega , hinting at a new Valkyria Chronicles game . In its September issue , Famitsu listed that Senjō no Valkyria 3 would be arriving on the PlayStation Portable . Its first public appearance was at the 2010 Tokyo Game Show ( TGS ) , where a demo was made available for journalists and attendees . During the publicity , story details were kept <unk> so as not to <unk> too much for potential players , along with some of its content still being in flux at the time of its reveal . To promote the game and detail the story leading into the game \\'s events , an episodic Flash visual novel written by <unk> began release in January 2011 . The game was released January 27 , 2011 . During an interview , the development team said that the game had the capacity for downloadable content ( DLC ) , but that no plans were finalized . Multiple DLC maps , featuring additional missions and <unk> characters , were released between February and April 2011 . An expanded edition of the game , Valkyria Chronicles III Extra Edition , released on November 23 , 2011 . <unk> and sold at a lower price than the original , Extra Edition game with seven additional episodes : three new , three chosen by staff from the game \\'s DLC , and one made available as a pre @-@ order bonus . People who also owned the original game could transfer their save data between versions . Unlike its two predecessors , Valkyria Chronicles III was not released in the west . According to Sega , this was due to poor sales of Valkyria Chronicles II and the general unpopularity of the PSP in the west . An unofficial fan translation patch began development in February 2012 : players with a copy of Valkyria Chronicles III could download and apply the patch , which translated the game \\'s text into English . <unk> with the Extra Edition , the patch was released in January 2014 . = = Reception = = On its day of release in Japan , Valkyria Chronicles III topped both platform @-@ exclusive and multi @-@ platform sales charts . By early February , the game sold 102 @,@ <unk> units , coming in second overall to The Last Story for the Wii . By the end of the year , the game had sold just over 152 @,@ 500 units . Famitsu enjoyed the story , and were particularly pleased with the improvements to gameplay . Japanese gaming site Game Watch <unk> , despite negatively noting its pacing and elements recycled from previous games , was generally positive about its story and characters , and found its gameplay entertaining despite off @-@ putting difficulty spikes . <unk> writer <unk> <unk> , in a \" Play Test \" article based on the game \\'s <unk> demo , felt that Valkyria Chronicles III provided a \" profound feeling of closure \" for the Valkyria Chronicles series . He praised its gameplay despite annoying limitations to aspects such as special abilities , and positively noted its shift in story to a tone similar to the first game . PlayStation Official Magazine - UK praised the story \\'s <unk> of Gallia \\'s moral standing , art style , and most points about its gameplay , positively noting the latter for both its continued quality and the tweaks to balance and content . Its one major criticism were multiple difficulty spikes , something that had affected the previous games . Heath Hindman of gaming website PlayStation <unk> praised the addition of non @-@ linear elements and improvements or removal of mechanics from Valkyria Chronicles II in addition to praising the returning gameplay style of previous games . He also positively noted the story \\'s serious tone . Points criticized in the review were recycled elements , awkward cutscenes that seemed to include all characters in a scene for no good reason , pacing issues , and occasional problems with the game \\'s AI . In a preview of the TGS demo , Ryan Geddes of IGN was left excited as to where the game would go after completing the demo , along with enjoying the improved visuals over Valkyria Chronicles II . Kotaku \\'s Richard <unk> was highly positive about the game , citing is story as a return to form after Valkyria Chronicles II and its gameplay being the best in the series . His main criticisms were its length and gameplay repetition , along with expressing regret that it would not be localized . = = Legacy = = Kurt and Riela were featured in the Nintendo 3DS crossover Project X Zone , representing the Valkyria series . Media.Vision would return to the series to develop Valkyria : Azure Revolution , with Ozawa returning as director . Azure Revolution is a role @-@ playing video game for the PlayStation 4 that forms the beginning of a new series within the Valkyria franchise . = = = Adaptations = = = Valkyria Chronicles 3 was adapted into a two @-@ episode original video animation series in the same year of its release . <unk> Senjō no Valkyria 3 : <unk> <unk> no <unk> ( <unk> <unk> , lit . Valkyria of the Battlefield 3 : The <unk> Taken for <unk> \\'s Sake ) , it was originally released through PlayStation Network and <unk> between April and May 2011 . The initially @-@ planned release and availability period needed to be extended due to a stoppage to <unk> during the early summer of that year . It later released for DVD on June 29 and August 31 , 2011 , with separate \" Black \" and \" Blue \" editions being available for purchase . The anime is set during the latter half of Valkyria Chronicles III , detailing a mission by the Nameless against their Imperial rivals Calamity Raven . The anime was first announced in November 2010 . It was developed by A @-@ 1 Pictures , produced by Shinji <unk> , directed by <unk> <unk> , and written by Hiroshi <unk> . Sakimoto \\'s music for the game was used in the anime . The anime \\'s title was inspired by the principle purpose of the Nameless : to suffer in battle for the goals of others . A <unk> attached to the project during development was \" The Road to <unk> \" , which referenced the <unk> Tank Museum in Moscow . The game \\'s main theme was how the characters regained their sense of self when stripped of their names and identities , along with general themes focused on war and its consequences . While making the anime , the production team were told by Sega to make it as realistic as possible , with the consequence that the team did extensive research into aspects such as what happened when vehicles like tanks were overturned or damaged . Due to it being along the same timeline as the original game and its television anime adaptation , the cast of Valkyria Chronicles could make appearances , which pleased the team . The opening theme , \" <unk> ( Light ) <unk> \" ( <unk> @-@ <unk> ) , was sung by Japanese singer <unk> . The ending theme , \" <unk> the Flowers of Light Will Bloom \" ( <unk> , <unk> <unk> <unk> no <unk> ) , was sung by <unk> <unk> . Both songs \\' lyrics were written by their respective artists . Two manga adaptations were produced , following each of the game \\'s main female protagonists Imca and Riela . They were Senjō no Valkyria 3 : <unk> <unk> <unk> no <unk> ( 戦場のヴァルキュリア3 <unk> , lit . Valkyria of the Battlefield 3 : The Flower of the Nameless Oath ) , illustrated by <unk> <unk> and eventually released in two volumes after being serialized in Dengeki <unk> between 2011 and 2012 ; and Senjō no Valkyria 3 : <unk> <unk> no <unk> <unk> ( 戦場のヴァルキュリア3 <unk> , lit . Valkyria of the Battlefield 3 <unk> <unk> of the Crimson Fate ) , illustrated by <unk> <unk> and eventually released in a single volume by Kadokawa Shoten in 2012 .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_doc_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we use a lemmatizer from `nltk`. In the list comprehension, we implement a simple rule: only consider words that are longer than 2 characters, start with a letter and match the `token_pattern`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [\n",
    "            self.wnl.lemmatize(t)\n",
    "            for t in doc.split()\n",
    "            if len(t) >= 2 and re.match(\"[a-z].*\", t) and re.match(token_pattern, t)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform lemmatizing and counting next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing and counting, this may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 16650\n",
      "Done. Time elapsed: 12.35s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Lemmatizing and counting, this may take a few minutes...\")\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(\n",
    "    input=\"content\",\n",
    "    analyzer=\"word\",\n",
    "    stop_words=\"english\",\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    max_df=0.9,\n",
    "    min_df=3,\n",
    ")\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_doc_list)\n",
    "val_vectors = vectorizer.transform(val_doc_list)\n",
    "test_vectors = vectorizer.transform(test_doc_list)\n",
    "\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "vocab_size = len(vocab_list)\n",
    "print(\"vocab size:\", vocab_size)\n",
    "print(\"Done. Time elapsed: {:.2f}s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all the parameters (weights and biases) in the NTM model are `np.float32` type we'd need the input data to also be in `np.float32`. It is better to do this type-casting upfront rather than repeatedly casting during mini-batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "\n",
    "def shuffle_and_dtype(vectors):\n",
    "    idx = np.arange(vectors.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    vectors = vectors[idx]\n",
    "    vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "    print(type(vectors), vectors.dtype)\n",
    "    return vectors\n",
    "\n",
    "\n",
    "train_vectors = shuffle_and_dtype(train_vectors)\n",
    "val_vectors = shuffle_and_dtype(val_vectors)\n",
    "test_vectors = shuffle_and_dtype(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NTM algorithm, as well as other first-party SageMaker algorithms, accepts data in [RecordIO](https://mxnet.apache.org/api/python/io/io.html#module-mxnet.recordio) [Protobuf](https://developers.google.com/protocol-buffers/) format. Here we define a helper function to convert the data to RecordIO Protobuf format. In addition, we will have the option to split the data into several parts specified by `n_parts`.\n",
    "\n",
    "The algorithm inherently supports multiple files in the training folder (\"channel\"), which could be very helpful for large data sets. In addition, when we use distributed training with multiple workers (compute instances), having multiple files allows us to distribute different portions of the training data to different workers conveniently.\n",
    "\n",
    "Inside this helper function we use `write_spmatrix_to_sparse_tensor` function provided by [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) to convert scipy sparse matrix into RecordIO Protobuf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_convert(sparray, prefix, fname_template=\"data_part{}.pbr\", n_parts=2):\n",
    "    import io\n",
    "    import sagemaker.amazon.common as smac\n",
    "\n",
    "    chunk_size = sparray.shape[0] // n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i * chunk_size\n",
    "        end = (i + 1) * chunk_size\n",
    "        if i + 1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "\n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        with open(fname, \"wb\") as f:\n",
    "            f.write(buf.getvalue())\n",
    "        print(\"Saved data to {}\".format(fname))\n",
    "\n",
    "\n",
    "train_data_dir = os.path.join(data_dir, \"train\")\n",
    "val_data_dir = os.path.join(data_dir, \"validation\")\n",
    "test_data_dir = os.path.join(data_dir, \"test\")\n",
    "\n",
    "check_create_dir(train_data_dir)\n",
    "check_create_dir(val_data_dir)\n",
    "check_create_dir(test_data_dir)\n",
    "\n",
    "split_convert(train_vectors, prefix=train_data_dir, fname_template=\"train_part{}.pbr\", n_parts=4)\n",
    "split_convert(val_vectors, prefix=val_data_dir, fname_template=\"val_part{}.pbr\", n_parts=1)\n",
    "split_convert(test_vectors, prefix=test_data_dir, fname_template=\"test_part{}.pbr\", n_parts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the vocabulary file\n",
    "To make use of the auxiliary channel for vocabulary file, we first save the text file with the name `vocab.txt` in the auxiliary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_data_dir = os.path.join(data_dir, \"auxiliary\")\n",
    "check_create_dir(aux_data_dir)\n",
    "with open(os.path.join(aux_data_dir, \"vocab.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in vocab_list:\n",
    "        f.write(item + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Data on S3\n",
    "\n",
    "Below we upload the data to an Amazon S3 destination for the model to access it during training.\n",
    "\n",
    "#### Setup AWS Credentials\n",
    "\n",
    "We first need to specify data locations and access roles. ***This is the only cell of this notebook that you will need to edit.*** In particular, we need the following data:\n",
    "\n",
    "- The S3 `bucket` and `prefix` that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM `role` is used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()  # <or insert your own bucket name>#\n",
    "prefix = \"ntm/\" + dataset\n",
    "\n",
    "train_prefix = os.path.join(prefix, \"train\")\n",
    "val_prefix = os.path.join(prefix, \"val\")\n",
    "aux_prefix = os.path.join(prefix, \"auxiliary\")\n",
    "test_prefix = os.path.join(prefix, \"test\")\n",
    "output_prefix = os.path.join(prefix, \"output\")\n",
    "\n",
    "s3_train_data = os.path.join(\"s3://\", bucket, train_prefix)\n",
    "s3_val_data = os.path.join(\"s3://\", bucket, val_prefix)\n",
    "s3_aux_data = os.path.join(\"s3://\", bucket, aux_prefix)\n",
    "s3_test_data = os.path.join(\"s3://\", bucket, test_prefix)\n",
    "output_path = os.path.join(\"s3://\", bucket, output_prefix)\n",
    "print(\"Training set location\", s3_train_data)\n",
    "print(\"Validation set location\", s3_val_data)\n",
    "print(\"Auxiliary data location\", s3_aux_data)\n",
    "print(\"Test data location\", s3_test_data)\n",
    "print(\"Trained model will be saved at\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the input directories to s3\n",
    "We use the `aws` command line interface (CLI) to upload the various input channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd_train = \"aws s3 cp \" + train_data_dir + \" \" + s3_train_data + \" --recursive\"\n",
    "p = subprocess.Popen(cmd_train, shell=True, stdout=subprocess.PIPE)\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_val = \"aws s3 cp \" + val_data_dir + \" \" + s3_val_data + \" --recursive\"\n",
    "p = subprocess.Popen(cmd_val, shell=True, stdout=subprocess.PIPE)\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_test = \"aws s3 cp \" + test_data_dir + \" \" + s3_test_data + \" --recursive\"\n",
    "p = subprocess.Popen(cmd_test, shell=True, stdout=subprocess.PIPE)\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_aux = \"aws s3 cp \" + aux_data_dir + \" \" + s3_aux_data + \" --recursive\"\n",
    "p = subprocess.Popen(cmd_aux, shell=True, stdout=subprocess.PIPE)\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "We have prepared the train, validation, test and auxiliary input channels on s3. Next, we configure a SageMaker training job to use the NTM algorithm on the data we prepared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker uses Amazon Elastic Container Registry (ECR) docker container to host the NTM training image. The following ECR containers are currently available for SageMaker NTM training in different regions. For the latest Docker container registry please refer to [Amazon SageMaker: Common Parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, \"ntm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below automatically chooses an algorithm container based on the current region. In the API call to `sagemaker.estimator.Estimator` we also specify the type and count of instances for the training job. Because the wikitext-2 data set is relatively small, we have chosen a CPU only instance (`ml.c4.xlarge`), but do feel free to change to [other instance types](https://aws.amazon.com/sagemaker/pricing/instance-types/). NTM fully takes advantage of GPU hardware and in general trains roughly an order of magnitude faster on a GPU than on a CPU. Multi-GPU or multi-instance training further improves training speed roughly linearly if communication overhead is low compared to compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the hyperparameters, including the newly introduced `sub_sample`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "ntm.set_hyperparameters(\n",
    "    num_topics=num_topics, feature_dim=vocab_size, mini_batch_size=60, epochs=50, sub_sample=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify how the data will be distributed to the workers during training as well as their content type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input\n",
    "\n",
    "s3_train = s3_input(\n",
    "    s3_train_data, distribution=\"ShardedByS3Key\", content_type=\"application/x-recordio-protobuf\"\n",
    ")\n",
    "s3_val = s3_input(\n",
    "    s3_val_data, distribution=\"FullyReplicated\", content_type=\"application/x-recordio-protobuf\"\n",
    ")\n",
    "s3_test = s3_input(\n",
    "    s3_test_data, distribution=\"FullyReplicated\", content_type=\"application/x-recordio-protobuf\"\n",
    ")\n",
    "\n",
    "s3_aux = s3_input(s3_aux_data, distribution=\"FullyReplicated\", content_type=\"text/plain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run the training job. Again, we will notice in the log that the top words are printed together with the WETC and TU scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm.fit({\"train\": s3_train, \"validation\": s3_val, \"auxiliary\": s3_aux, \"test\": s3_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is completed, you can view information about and the status of a training job using the AWS SageMaker console. Just click on the \"Jobs\" tab and select training job matching the training job name, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training job name: {}\".format(ntm.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate the utility of the `sub_sample` hyperparameter by setting it to 1.0 and 0.2 for training on the wikitext-103 dataset (simply change the dataset name and download URL, re-start the kernel and re-run this notebook). In both settings, we set `epochs = 100` and NTM would early-exit training when the loss on validation data does not improve in 3 consecutive epochs. We report the TU, WETC, NPMI of the best epoch based on validation loss as well as the total time for both settings below. Note we ran each training job on a single GPU of a `p2.8xlarge` machine.\n",
    "![subsample_result_table](subsample_table.png)\n",
    "We observe that setting sub_sample to 0.2 leads to reduced total training time even though it takes more epochs to converge (49 instead of 18). The increase in the number of epochs to convergence is expected due to the variance introduced by training on a random subset of data per epoch. Yet the overall training time is reduced because training is about 5 times faster per epoch at the subsampling rate of 0.2. We also note the higher scores in terms of TU, WETC and NPMI at the end of training with subsampling. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this blog post, we introduced 3 new features of SageMaker NTM algorithm. Upon reading this blog and completing the new sample notebook, you should have learned how to add an auxiliary vocabulary channel to automatically map integer representations of words in the detected topics to a human understandable vocabulary. You also have learned to evaluate the quality of the a model using both Word Embedding Topic Coherence and Topic Uniqueness metrics. Lastly, you learned to use the subsampling feature to reduce the model training time while maintaining similar model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
